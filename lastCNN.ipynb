{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision\nfrom torchvision import datasets, transforms\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader , ConcatDataset , Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch.nn.functional as F\nfrom PIL import Image\n\nimport os\nprint(os.listdir(\"/kaggle/input/cell-images-for-detecting-malaria/cell_images/cell_images\"))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-07T19:10:06.297615Z","iopub.execute_input":"2024-05-07T19:10:06.298060Z","iopub.status.idle":"2024-05-07T19:10:15.660047Z","shell.execute_reply.started":"2024-05-07T19:10:06.298028Z","shell.execute_reply":"2024-05-07T19:10:15.658812Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"['Uninfected', 'Parasitized']\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-05-07T19:10:32.338066Z","iopub.execute_input":"2024-05-07T19:10:32.338492Z","iopub.status.idle":"2024-05-07T19:10:32.344715Z","shell.execute_reply.started":"2024-05-07T19:10:32.338461Z","shell.execute_reply":"2024-05-07T19:10:32.343246Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_transforms = transforms.Compose([\n    transforms.Resize((120, 120)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])","metadata":{"execution":{"iopub.status.busy":"2024-05-07T19:10:42.579767Z","iopub.execute_input":"2024-05-07T19:10:42.580979Z","iopub.status.idle":"2024-05-07T19:10:42.588307Z","shell.execute_reply.started":"2024-05-07T19:10:42.580938Z","shell.execute_reply":"2024-05-07T19:10:42.586916Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"image_dir = \"/kaggle/input/cell-images-for-detecting-malaria/cell_images/cell_images\"\ntrain_set = datasets.ImageFolder(image_dir, transform=train_transforms)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T19:10:58.194742Z","iopub.execute_input":"2024-05-07T19:10:58.195136Z","iopub.status.idle":"2024-05-07T19:11:36.203511Z","shell.execute_reply.started":"2024-05-07T19:10:58.195108Z","shell.execute_reply":"2024-05-07T19:11:36.202136Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"test_size = 0.2\nnum_train = len(train_set)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\ntest_split = int(np.floor(test_size * num_train))\ntest_index, train_index = indices[:test_split], indices[test_split:]\n\ntrain_sampler = SubsetRandomSampler(train_index)\ntest_sampler = SubsetRandomSampler(test_index)\n\ntrain_loader = DataLoader(train_set, sampler=train_sampler, batch_size=104)\ntest_loader = DataLoader(train_set, sampler=test_sampler, batch_size=58)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T19:11:40.097757Z","iopub.execute_input":"2024-05-07T19:11:40.098163Z","iopub.status.idle":"2024-05-07T19:11:40.110985Z","shell.execute_reply.started":"2024-05-07T19:11:40.098133Z","shell.execute_reply":"2024-05-07T19:11:40.109585Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, latent_dim, img_shape):\n        super(Generator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(latent_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Linear(256, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Linear(512, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Linear(1024, np.prod(img_shape)),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        img = self.model(z)\n        img = img.view(img.size(0), *img_shape)\n        return img","metadata":{"execution":{"iopub.status.busy":"2024-05-07T19:11:46.190087Z","iopub.execute_input":"2024-05-07T19:11:46.190510Z","iopub.status.idle":"2024-05-07T19:11:46.200717Z","shell.execute_reply.started":"2024-05-07T19:11:46.190478Z","shell.execute_reply":"2024-05-07T19:11:46.199500Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, img_shape):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(np.prod(img_shape), 512),\n            nn.LeakyReLU(0.2),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.LeakyReLU(0.2),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, img):\n        img_flat = img.view(img.size(0), -1)\n        validity = self.model(img_flat)\n        return validity","metadata":{"execution":{"iopub.status.busy":"2024-05-07T19:11:56.897434Z","iopub.execute_input":"2024-05-07T19:11:56.897881Z","iopub.status.idle":"2024-05-07T19:11:56.906661Z","shell.execute_reply.started":"2024-05-07T19:11:56.897851Z","shell.execute_reply":"2024-05-07T19:11:56.905283Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class GAN(nn.Module):\n    def __init__(self, generator, discriminator):\n        super(GAN, self).__init__()\n        self.generator = generator\n        self.discriminator = discriminator\n\n    def forward(self, z):\n        img = self.generator(z)\n        validity = self.discriminator(img)\n        return validity","metadata":{"execution":{"iopub.status.busy":"2024-05-07T19:12:12.035973Z","iopub.execute_input":"2024-05-07T19:12:12.036358Z","iopub.status.idle":"2024-05-07T19:12:12.043310Z","shell.execute_reply.started":"2024-05-07T19:12:12.036330Z","shell.execute_reply":"2024-05-07T19:12:12.042058Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\nlatent_dim = 128\nimg_shape = (3, 120, 120)\ngenerator = Generator(latent_dim, img_shape).to(device)\ndiscriminator = Discriminator(img_shape).to(device)\ngan = GAN(generator, discriminator)\nadversarial_loss = nn.BCELoss()\n\nnum_epochs_gan = 3\nlearning_rate_gan = 0.0002\nlearning_rate_classifier = 0.001\n\nbatch_size_gan = 64\ncriterion = nn.BCELoss()\noptimizer_gan = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizer_D =  optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\ngenerated_images_dir = \"/kaggle/working/generated_images\"","metadata":{"execution":{"iopub.status.busy":"2024-05-07T19:12:24.134581Z","iopub.execute_input":"2024-05-07T19:12:24.135029Z","iopub.status.idle":"2024-05-07T19:12:25.022395Z","shell.execute_reply.started":"2024-05-07T19:12:24.134997Z","shell.execute_reply":"2024-05-07T19:12:25.021095Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"if not os.path.exists(generated_images_dir):\n    os.makedirs(generated_images_dir)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T19:12:38.387058Z","iopub.execute_input":"2024-05-07T19:12:38.387467Z","iopub.status.idle":"2024-05-07T19:12:38.393071Z","shell.execute_reply.started":"2024-05-07T19:12:38.387437Z","shell.execute_reply":"2024-05-07T19:12:38.391825Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def save_generated_images(images, epoch):\n    for i, image in enumerate(images):\n        save_path = os.path.join(generated_images_dir, f\"generated_image_epoch_{epoch}_index_{i}.png\")\n        torchvision.utils.save_image(image, save_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T19:12:54.591830Z","iopub.execute_input":"2024-05-07T19:12:54.592207Z","iopub.status.idle":"2024-05-07T19:12:54.598955Z","shell.execute_reply.started":"2024-05-07T19:12:54.592180Z","shell.execute_reply":"2024-05-07T19:12:54.597518Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class MosquitoNet(nn.Module):\n    \n    def __init__(self):\n        super(MosquitoNet, self).__init__()\n        \n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.layer3 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n            \n        self.fc1 = nn.Linear(64*15*15, 512)\n        self.fc2 = nn.Linear(512, 128)\n        self.fc3 = nn.Linear(128, 2)\n        self.drop = nn.Dropout2d(0.2)\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = out.view(out.size(0), -1)    # flatten out a input for Dense Layer\n        out = self.fc1(out)\n        out = F.relu(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = F.relu(out)\n        out = self.drop(out)\n        out = self.fc3(out)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2024-05-07T19:13:10.445529Z","iopub.execute_input":"2024-05-07T19:13:10.445951Z","iopub.status.idle":"2024-05-07T19:13:10.459518Z","shell.execute_reply.started":"2024-05-07T19:13:10.445921Z","shell.execute_reply":"2024-05-07T19:13:10.458403Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class GeneratedDataset(torch.utils.data.Dataset):\n    def __init__(self, generated_dir, transform=None):\n        self.generated_dir = generated_dir\n        self.generated_files = os.listdir(generated_dir)\n        self.transform = transform\n\n    def __getitem__(self, index):\n        img_path = os.path.join(self.generated_dir, self.generated_files[index])\n        img = Image.open(img_path)\n        if self.transform:\n            img = self.transform(img)\n        return img , 0\n\n    def __len__(self):\n        return len(self.generated_files)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T19:13:47.197301Z","iopub.execute_input":"2024-05-07T19:13:47.197829Z","iopub.status.idle":"2024-05-07T19:13:47.207051Z","shell.execute_reply.started":"2024-05-07T19:13:47.197798Z","shell.execute_reply":"2024-05-07T19:13:47.205784Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"num_generated_images = 0\nfor epoch in range(num_epochs_gan):\n    for i,(imgs,_) in enumerate(train_loader, 0):\n        valid = torch.ones(imgs.size(0), 1).to(device)\n        fake = torch.zeros(imgs.size(0), 1).to(device)\n\n        # Configure input\n        real_imgs = imgs.to(device)\n\n        # -----------------\n        #  Train Generator\n        # -----------------\n\n        optimizer_gan.zero_grad()\n\n        # Sample noise as generator input\n        z = torch.randn(imgs.size(0), latent_dim).to(device)\n\n        # Generate a batch of images\n        gen_imgs = generator(z)\n        save_generated_images(gen_imgs, epoch)\n\n        # Loss measures generator's ability to fool the discriminator\n        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n\n        g_loss.backward()\n        optimizer_gan.step()\n\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n\n        optimizer_D.zero_grad()\n\n        # Measure discriminator's ability to classify real from generated samples\n        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n        d_loss = (real_loss + fake_loss) / 2\n\n        d_loss.backward()\n        optimizer_D.step()\n\n\n        \n\n# Save the trained generator model\n\ntorch.save(generator.state_dict(), \"/kaggle/working/model.pt\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T19:13:56.011337Z","iopub.execute_input":"2024-05-07T19:13:56.011773Z","iopub.status.idle":"2024-05-07T19:39:00.715426Z","shell.execute_reply.started":"2024-05-07T19:13:56.011736Z","shell.execute_reply":"2024-05-07T19:39:00.714099Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"generated_dataset = GeneratedDataset(generated_images_dir, transform=train_transforms)\n\n# Check the length of the generated dataset\nprint(\"Length of generated dataset:\", len(generated_dataset))\n\ngenerated_loader = DataLoader(generated_dataset, batch_size=104, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T19:39:44.726190Z","iopub.execute_input":"2024-05-07T19:39:44.726630Z","iopub.status.idle":"2024-05-07T19:39:44.734670Z","shell.execute_reply.started":"2024-05-07T19:39:44.726596Z","shell.execute_reply":"2024-05-07T19:39:44.733347Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Length of generated dataset: 312\n","output_type":"stream"}]},{"cell_type":"code","source":"class CombinedDataset(Dataset):\n    def __init__(self, original_dataset, generated_dataset, transform=None):\n        self.original_dataset = original_dataset\n        self.generated_dataset = generated_dataset\n        self.transform = transform\n\n    def __getitem__(self, index):\n        if index < len(self.original_dataset):\n            original_item, original_label = self.original_dataset[index]\n            if self.transform:\n                original_item = self.transform(original_item)\n            return original_item, original_label\n        else:\n            generated_item, _ = self.generated_dataset[index - len(self.original_dataset)]\n            if self.transform:\n                generated_item = self.transform(generated_item)\n            return generated_item, 0\n\n    def __len__(self):\n        return len(self.original_dataset) + len(self.generated_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T19:40:01.262941Z","iopub.execute_input":"2024-05-07T19:40:01.263414Z","iopub.status.idle":"2024-05-07T19:40:01.274276Z","shell.execute_reply.started":"2024-05-07T19:40:01.263380Z","shell.execute_reply":"2024-05-07T19:40:01.272642Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"combined_dataset = CombinedDataset(train_set, generated_dataset)\n\n# Create a combined data loader\ncombined_loader = DataLoader(combined_dataset, batch_size=batch_size_gan, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T19:40:19.610994Z","iopub.execute_input":"2024-05-07T19:40:19.611366Z","iopub.status.idle":"2024-05-07T19:40:19.617640Z","shell.execute_reply.started":"2024-05-07T19:40:19.611337Z","shell.execute_reply":"2024-05-07T19:40:19.616235Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"correct = 0\ntotal = 0\nclass_total = [0 for _ in range(2)]\nclass_correct = [0 for _ in range(2)]\nbatch_size = 58\n\n# Lists used in Confusion Matrix\nactual = []\npredict = []","metadata":{"execution":{"iopub.status.busy":"2024-05-07T19:43:28.656797Z","iopub.execute_input":"2024-05-07T19:43:28.657246Z","iopub.status.idle":"2024-05-07T19:43:28.663583Z","shell.execute_reply.started":"2024-05-07T19:43:28.657213Z","shell.execute_reply":"2024-05-07T19:43:28.662269Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model = MosquitoNet()\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T19:43:40.703427Z","iopub.execute_input":"2024-05-07T19:43:40.703855Z","iopub.status.idle":"2024-05-07T19:43:40.781753Z","shell.execute_reply.started":"2024-05-07T19:43:40.703824Z","shell.execute_reply":"2024-05-07T19:43:40.780596Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"MosquitoNet(\n  (layer1): Sequential(\n    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (layer2): Sequential(\n    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (layer3): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (fc1): Linear(in_features=14400, out_features=512, bias=True)\n  (fc2): Linear(in_features=512, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=2, bias=True)\n  (drop): Dropout2d(p=0.2, inplace=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"error = nn.CrossEntropyLoss()\nlearning_rate = 0.001\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training loop\nnum_epochs = 20\nbatch_size = 100","metadata":{"execution":{"iopub.status.busy":"2024-05-07T19:43:58.077549Z","iopub.execute_input":"2024-05-07T19:43:58.077998Z","iopub.status.idle":"2024-05-07T19:43:58.085027Z","shell.execute_reply.started":"2024-05-07T19:43:58.077965Z","shell.execute_reply":"2024-05-07T19:43:58.083637Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train_losses = []\ntrain_accuracies = []\nfor epoch in range(num_epochs):\n    train_loss = 0.\n    model.train()    # explicitly stating the training\n    \n    for i, (images, labels) in enumerate(combined_loader):\n        images, labels = images.to(device), labels.to(device)\n        train = images.view(-1, 3, 120, 120)\n        outputs = model(train)\n        \n        optimizer.zero_grad()\n        loss = error(outputs, labels)\n        loss.backward()    # back-propagation\n        optimizer.step()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n     \n        accuracy = correct / total\n        train_losses.append(train_loss / len(train_loader.dataset))\n        train_accuracies.append(accuracy)\n     \n    \n\n# Save the trained model\ntorch.save(model.state_dict(), \"/kaggle/working/modeltwo.pt\")","metadata":{"execution":{"iopub.status.busy":"2024-05-07T19:44:11.935893Z","iopub.execute_input":"2024-05-07T19:44:11.936317Z","iopub.status.idle":"2024-05-07T21:58:01.052524Z","shell.execute_reply.started":"2024-05-07T19:44:11.936287Z","shell.execute_reply":"2024-05-07T21:58:01.051095Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n  warnings.warn(warn_msg)\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report, f1_score\n\ncorrect = 0\ntotal = 0\nclass_total = [0 for _ in range(2)]\nclass_correct = [0 for _ in range(2)]\nbatch_size = 58\n\n# Lists used in Confusion Matrix\nactual = []\npredict = []\n\nmodel.eval()    # explicitly stating the testing \nwith torch.no_grad():\n    for i, (images, labels) in enumerate(combined_loader):\n        images, labels = images.to(device), labels.to(device)\n    \n        actual.append(labels.data.tolist())\n        test = images.view(-1, 3, 120, 120)\n        outputs = model(test)\n        predicted = torch.max(outputs, 1)[1]\n        predict.append(predicted.data.tolist())\n        total += len(labels)\n        correct += (predicted == labels).sum().item()\n        \n        # Calculating classwise accuracy\n        c = (predicted == labels).squeeze().bool()\n        for pred, label in zip(predicted, labels):\n            class_correct[label.item()] += (pred == label).item()\n            class_total[label.item()] += 1\n            actual_flat = [item for sublist in actual for item in sublist]\n            predict_flat = [item for sublist in predict for item in sublist]\n\nconf_matrix = confusion_matrix(actual_flat, predict_flat)\n\n# Compute F1 score\nf1 = f1_score(actual_flat, predict_flat, average='weighted')\n\n\n# Generate classification report\nclass_report = classification_report(actual_flat, predict_flat)                \nprint(\"Accuracy on the Test set: {:.2f}%\".format(correct * 100 / total))\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\nprint(\"\\nF1 Score:\", f1)\nprint(\"\\nClassification Report:\")\nprint(class_report)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T22:25:13.072607Z","iopub.execute_input":"2024-05-07T22:25:13.073695Z","iopub.status.idle":"2024-05-07T22:29:12.353065Z","shell.execute_reply.started":"2024-05-07T22:25:13.073659Z","shell.execute_reply":"2024-05-07T22:29:12.351807Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n  warnings.warn(warn_msg)\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on the Test set: 99.52%\nConfusion Matrix:\n[[14022    69]\n [   66 13713]]\n\nF1 Score: 0.9951560875901019\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     14091\n           1       0.99      1.00      1.00     13779\n\n    accuracy                           1.00     27870\n   macro avg       1.00      1.00      1.00     27870\nweighted avg       1.00      1.00      1.00     27870\n\n","output_type":"stream"}]}]}